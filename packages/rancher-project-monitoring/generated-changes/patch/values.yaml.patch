--- charts-original/values.yaml
+++ charts/values.yaml
@@ -1,567 +1,8 @@
-# Default values for kube-prometheus-stack.
+# Default values for project-prometheus-stack.
 # This is a YAML-formatted file.
 # Declare variables to be passed into your templates.
 
-# Rancher Monitoring Configuration
-
-## Configuration for prometheus-adapter
-## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter
-##
-prometheus-adapter:
-  enabled: true
-  prometheus:
-    # Change this if you change the namespaceOverride or nameOverride of prometheus-operator
-    url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
-    port: 9090
-
-## RKE PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-pushprox
-##
-rkeControllerManager:
-  enabled: false
-  metricsPort: 10257 # default to secure port as of k8s >= 1.22
-  component: kube-controller-manager
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10011
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/controlplane: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10252 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rkeScheduler:
-  enabled: false
-  metricsPort: 10259
-  component: kube-scheduler
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10012
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/controlplane: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.23"
-    values:
-      metricsPort: 10251 # default to insecure port in k8s < 1.23
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rkeProxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-rkeEtcd:
-  enabled: false
-  metricsPort: 2379
-  component: kube-etcd
-  clients:
-    port: 10014
-    https:
-      enabled: true
-      certDir: /etc/kubernetes/ssl
-      certFile: kube-etcd-*.pem
-      keyFile: kube-etcd-*-key.pem
-      caCertFile: kube-ca.pem
-      seLinuxOptions:
-        # Gives rkeEtcd permissions to read files in /etc/kubernetes/*
-        # Type is defined in https://github.com/rancher/rancher-selinux
-        type: rke_kubereader_t
-    nodeSelector:
-      node-role.kubernetes.io/etcd: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-rkeIngressNginx:
-  enabled: false
-  metricsPort: 10254
-  component: ingress-nginx
-  clients:
-    port: 10015
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-    nodeSelector:
-      node-role.kubernetes.io/worker: "true"
-
-## k3s PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-pushprox
-##
-k3sServer:
-  enabled: false
-  metricsPort: 10250
-  component: k3s-server
-  clients:
-    port: 10013
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    rbac:
-      additionalRules:
-      - nonResourceURLs: ["/metrics/cadvisor"]
-        verbs: ["get"]
-      - apiGroups: [""]
-        resources: ["nodes/metrics"]
-        verbs: ["get"]
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  serviceMonitor:
-    endpoints:
-    - port: metrics
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/cadvisor
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/probes
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-
-hardened:
-  k3s:  
-    networkPolicy:
-      enabled: true
-
-## KubeADM PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-pushprox
-##
-kubeAdmControllerManager:
-  enabled: false
-  metricsPort: 10257
-  component: kube-controller-manager
-  clients:
-    port: 10011
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmScheduler:
-  enabled: false
-  metricsPort: 10259
-  component: kube-scheduler
-  clients:
-    port: 10012
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmProxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmEtcd:
-  enabled: false
-  metricsPort: 2381
-  component: kube-etcd
-  clients:
-    port: 10014
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-## rke2 PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-pushprox
-##
-rke2ControllerManager:
-  enabled: false
-  metricsPort: 10257 # default to secure port as of k8s >= 1.22
-  component: kube-controller-manager
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10011
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10252 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rke2Scheduler:
-  enabled: false
-  metricsPort: 10259 # default to secure port as of k8s >= 1.22
-  component: kube-scheduler
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10012
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10251 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rke2Proxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-rke2Etcd:
-  enabled: false
-  metricsPort: 2381
-  component: kube-etcd
-  clients:
-    port: 10014
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/etcd: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-rke2IngressNginx:
-  enabled: false
-  metricsPort: 10254
-  component: ingress-nginx
-  # in the RKE2 cluster, the ingress-nginx-controller is deployed
-  # as a non-hostNetwork workload starting at the following versions
-  # - >= v1.22.12+rke2r1 < 1.23.0-0
-  # - >= v1.23.9+rke2r1 < 1.24.0-0
-  # - >= v1.24.3+rke2r1 < 1.25.0-0
-  # - >= v1.25.0+rke2r1
-  # As a result we do not need clients and proxies as we can directly create
-  # a service that targets the workload with the given app name
-  namespaceOverride: kube-system
-  clients:
-    enabled: false
-  proxy:
-    enabled: false
-  service:
-    selector:
-      app.kubernetes.io/name: rke2-ingress-nginx
-  kubeVersionOverrides:
-  - constraint: "< 1.21.0-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a DaemonSet with 1 pod when RKE2 version is < 1.21.0-0
-        deployment:
-          enabled: false
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.21.0-0 < 1.22.12-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.21.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.23.0-0 < v1.23.9-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.24.0-0 < v1.24.3-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-
-
-
-## Additional PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-pushprox
-##
-
-# hardenedKubelet can only be deployed if kubelet.enabled=true
-# If enabled, it replaces the ServiceMonitor deployed by the default kubelet option with a 
-# PushProx-based exporter that does not require a host port to be open to scrape metrics.
-hardenedKubelet:
-  enabled: false
-  metricsPort: 10250
-  component: kubelet
-  clients:
-    port: 10015
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    rbac:
-      additionalRules:
-      - nonResourceURLs: ["/metrics/cadvisor"]
-        verbs: ["get"]
-      - apiGroups: [""]
-        resources: ["nodes/metrics"]
-        verbs: ["get"]
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  serviceMonitor:
-    endpoints:
-    - port: metrics
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/cadvisor
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/probes
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-
-# hardenedNodeExporter can only be deployed if nodeExporter.enabled=true
-# If enabled, it replaces the ServiceMonitor deployed by the default nodeExporter with a 
-# PushProx-based exporter that does not require a host port to be open to scrape metrics.
-hardenedNodeExporter:
-  enabled: false
-  metricsPort: 9796
-  component: node-exporter
-  clients:
-    port: 10016
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-## Upgrades
-upgrade:
-  ## Run upgrade scripts before an upgrade or rollback via a Job hook
-  enabled: true
-  ## Image to use to run the scripts
-  image:
-    repository: rancher/shell
-    tag: v0.1.22
-
+# Rancher Project Monitoring Configuration
 ## Rancher Monitoring
 ##
 
@@ -580,57 +21,15 @@
   ##
   selector: {}
 
-## Component scraping nginx-ingress-controller
-##
-ingressNginx:
-  enabled: false
-
-  ## The namespace to search for your nginx-ingress-controller
-  ##
-  namespace: ingress-nginx
-  
-  service:
-    port: 9913
-    targetPort: 10254
-    # selector:
-    #   app: ingress-nginx
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: "30s"
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## 	metric relabel configs to apply to samples before ingestion.
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    # 	relabel configs to apply to samples before ingestion.
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-# Prometheus Operator Configuration
-
-## Provide a name in place of kube-prometheus-stack for `app:` labels
+## Provide a name in place of project-prometheus-stack for `app:` labels
 ## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-nameOverride: "rancher-monitoring"
+nameOverride: "rancher-project-monitoring"
 
 ## Override the deployment namespace
 ## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-namespaceOverride: "cattle-monitoring-system"
+namespaceOverride: ""
 
 ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
 ##
@@ -655,33 +54,11 @@
 defaultRules:
   create: true
   rules:
-    alertmanager: true
-    etcd: true
-    configReloaders: true
     general: true
-    k8s: true
-    kubeApiserverAvailability: true
-    kubeApiserverBurnrate: true
-    kubeApiserverHistogram: true
-    kubeApiserverSlos: true
-    kubeControllerManager: true
-    kubelet: true
-    kubeProxy: true
-    kubePrometheusGeneral: true
-    kubePrometheusNodeRecording: true
+    prometheus: true
+    alertmanager: true
     kubernetesApps: true
-    kubernetesResources: true
     kubernetesStorage: true
-    kubernetesSystem: true
-    kubeSchedulerAlerting: true
-    kubeSchedulerRecording: true
-    kubeStateMetrics: true
-    network: true
-    node: true
-    nodeExporterAlerting: true
-    nodeExporterRecording: true
-    prometheus: true
-    prometheusOperator: true
 
   ## Reduce app namespace alert scope
   appNamespacesTarget: ".*"
@@ -702,60 +79,54 @@
 
   ## Disabled PrometheusRule alerts
   disabled: {}
-  # KubeAPIDown: true
-  # NodeRAIDDegraded: true
-
-## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
-##
-# additionalPrometheusRules: []
-#  - name: my-rule-file
-#    groups:
-#      - name: my_group
-#        rules:
-#        - record: my_record
-#          expr: 100 * my_record
-
-## Provide custom recording or alerting rules to be deployed into the cluster.
-##
-additionalPrometheusRulesMap: {}
-#  rule-name:
-#    groups:
-#    - name: my_group
-#      rules:
-#      - record: my_record
-#        expr: 100 * my_record
 
 ##
 global:
   cattle:
     psp:
       enabled: false
-
     systemDefaultRegistry: ""
-    ## Windows Monitoring
-    ## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-windows-exporter
-    ##
-    ## Deploys a DaemonSet of Prometheus exporters based on https://github.com/prometheus-community/windows_exporter.
-    ## Every Windows host must have a wins version of 0.1.0+ to use this chart (default as of Rancher 2.5.8).
-    ## To upgrade wins versions on Windows hosts, see https://github.com/rancher/wins/tree/master/charts/rancher-wins-upgrader.
-    ##
-    windows:
-      enabled: false
-  seLinux:
-    enabled: false
+    projectNamespaceSelector: {}
+    projectNamespaces: []
   kubectl:
      repository: rancher/kubectl
      tag: v1.20.2
      pullPolicy: IfNotPresent
+     securityContext:
+       runAsNonRoot: true
+       runAsUser: 1000
+  networkPolicy:
+    # If activated, creates ingress network policies to only allow ingress traffic from workloads within the project.
+    # This only works correctly, if Project Network Isolation is activated for the cluster in Rancher. Otherwise,
+    # Ingress traffic from the nodes and thus from the Kubernetes API will be blocked, which breaks accessing the UIs
+    # through the Rancher/Kubernetes API Proxy in the Rancher UI.
+    limitIngressToProject: false
+    # Custom ingress restrictions. If null and limitIngressToProject=false, all ingress traffic will be allowed.
+    ingress: null
+    # By default, all egress traffic is allowed.
+    egress:
+      - {}
   rbac:
     ## Create RBAC resources for ServiceAccounts and users
     ##
     create: true
 
     userRoles:
-      ## Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets
+      ## Create default user Roles that the Helm Project Operator will automatically create RoleBindings for
+      ##
+      ## How does this work?
+      ##
+      ## The operator will watch for all subjects bound to each Kubernetes default ClusterRole in the project registration namespace
+      ## where the ProjectHelmChart that deployed this chart belongs to; if it observes a subject bound to a particular role in
+      ## the project registration namespace (e.g. edit) and if a Role exists that is deployed by this chart with the label
+      ## 'helm.cattle.io/project-helm-chart-role-aggregate-from': '<role, e.g. edit>', it will automaticaly create a RoleBinding
+      ## in the release namespace binding all such subjects to that Role.
+      ##
+      ## Note: while the default behavior is to use the Kubernetes default ClusterRole, the operator deployment (prometheus-federator)
+      ## can be configured to use a different set of ClusterRoles as the source of truth for admin, edit, and view permissions.
+      ##
       create: true
-      ## Aggregate default user ClusterRoles into default k8s ClusterRoles
+      ## Add labels to Roles
       aggregateToDefaultRoles: true
 
     pspAnnotations: {}
@@ -768,10 +139,6 @@
       # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
       # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
 
-  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
-  ##
-  imageRegistry: docker.io
-
   ## Reference to one or more secrets to be used when pulling images
   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
   ##
@@ -780,6 +147,19 @@
   # or
   # - "image-pull-secret"
 
+federate:
+  ## enabled indicates whether to add federation to any Project Prometheus Stacks by default
+  ## If not enabled, no federation will be turned on
+  enabled: true
+
+  # Change this to point at all Prometheuses you want all your Project Prometheus Stacks to federate from
+  # By default, this matches the default deployment of Rancher Monitoring
+  targets:
+  - rancher-monitoring-prometheus.cattle-monitoring-system.svc:9090
+
+  ## Scrape interval
+  interval: "15s"
+
 ## Configuration for alertmanager
 ## ref: https://prometheus.io/docs/alerting/alertmanager/
 ##
@@ -804,7 +184,6 @@
     create: true
     name: ""
     annotations: {}
-    automountServiceAccountToken: true
 
   ## Configure pod disruption budgets for Alertmanager
   ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
@@ -859,13 +238,6 @@
     templates:
     - '/etc/alertmanager/config/*.tmpl'
 
-  ## Alertmanager configuration directives (as string type, preferred over the config hash map)
-  ## stringConfig will be used only, if tplConfig is true
-  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
-  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
-  ##
-  stringConfig: ""
-
   ## Pass the Alertmanager configuration directives through Helm's templating
   ## engine. If the Alertmanager configuration contains Alertmanager templates,
   ## they'll need to be properly escaped so that they are not interpreted by
@@ -967,11 +339,8 @@
 
     labels: {}
 
-    ## Override ingress to a different defined port on the service
+    ## Redirect ingress to an additional defined port on the service
     # servicePort: 8081
-    ## Override ingress to a different service then the default, this is useful if you need to
-    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)
-    # serviceName: kube-prometheus-stack-alertmanager-0
 
     ## Hosts must be provided if Ingress is enabled.
     ##
@@ -1000,53 +369,6 @@
   secret:
     annotations: {}
 
-  # by default the alertmanager secret is not overwritten if it already exists
-    recreateIfExists: false
-
-  ## Configuration for creating an Ingress that will map to each Alertmanager replica service
-  ## alertmanager.servicePerReplica must be enabled
-  ##
-  ingressPerReplica:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-
-    ## Final form of the hostname for each per replica ingress is
-    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
-    ##
-    ## Prefix for the per replica ingress that will have `-$replicaNumber`
-    ## appended to the end
-    hostPrefix: ""
-    ## Domain that will be used for the per replica ingress
-    hostDomain: ""
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## Secret name containing the TLS certificate for alertmanager per replica ingress
-    ## Secret must be manually created in the namespace
-    tlsSecretName: ""
-
-    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
-    ##
-    tlsSecretPerReplica:
-      enabled: false
-      ## Final form of the secret for each per replica ingress is
-      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
-      ##
-      prefix: "alertmanager"
-
   ## Configuration for Alertmanager service
   ##
   service:
@@ -1083,41 +405,6 @@
     ##
     externalTrafficPolicy: Cluster
 
-    ## If you want to make sure that connections from a particular client are passed to the same Pod each time
-    ## Accepts 'ClientIP' or ''
-    ##
-    sessionAffinity: ""
-
-    ## Service type
-    ##
-    type: ClusterIP
-
-  ## Configuration for creating a separate Service for each statefulset Alertmanager replica
-  ##
-  servicePerReplica:
-    enabled: false
-    annotations: {}
-
-    ## Port for Alertmanager Service per replica to listen on
-    ##
-    port: 9093
-
-    ## To be used with a proxy extraContainer port
-    targetPort: 9093
-
-    ## Port to expose on each node
-    ## Only used if servicePerReplica.type is 'NodePort'
-    ##
-    nodePort: 30904
-
-    ## Loadbalancer source IP ranges
-    ## Only used if servicePerReplica.type is "LoadBalancer"
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
     ## Service type
     ##
     type: ClusterIP
@@ -1130,30 +417,6 @@
     interval: ""
     selfMonitor: true
 
-    ## Additional labels
-    ##
-    additionalLabels: {}
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
     ## proxyUrl: URL of a proxy that should be used for scraping.
     ##
     proxyUrl: ""
@@ -1161,10 +424,6 @@
     ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
     scheme: ""
 
-    ## enableHttp2: Whether to enable HTTP2.
-    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
-    enableHttp2: true
-
     ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
     ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
     tlsConfig: {}
@@ -1203,7 +462,7 @@
     ##
     image:
       repository: rancher/mirrored-prometheus-alertmanager
-      tag: v0.25.0
+      tag: v0.26.0
       sha: ""
 
     ## If true then the user will be responsible to provide a secret with alertmanager configuration
@@ -1232,39 +491,13 @@
 
     ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
     ##
-    alertmanagerConfigSelector: {}
-    ## Example which selects all alertmanagerConfig resources
-    ## with label "alertconfig" with values any of "example-config" or "example-config-2"
-    # alertmanagerConfigSelector:
-    #   matchExpressions:
-    #     - key: alertconfig
-    #       operator: In
-    #       values:
-    #         - example-config
-    #         - example-config-2
-    #
-    ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
-    # alertmanagerConfigSelector:
-    #   matchLabels:
-    #     role: example-config
-
-    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
-    ##
-    alertmanagerConfigNamespaceSelector: {}
-    ## Example which selects all namespaces
-    ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
-    # alertmanagerConfigNamespaceSelector:
-    #   matchExpressions:
-    #     - key: alertmanagerconfig
-    #       operator: In
-    #       values:
-    #         - example-namespace
-    #         - example-namespace-2
-
-    ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
-    # alertmanagerConfigNamespaceSelector:
-    #   matchLabels:
-    #     alertmanagerconfig: enabled
+    alertmanagerConfigSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## AlermanagerConfig to be used as top level configuration
     ##
@@ -1273,13 +506,6 @@
     # alertmanagerConfiguration:
     #   name: global-alertmanager-Configuration
 
-    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:
-    ##
-    alertmanagerConfigMatcherStrategy: {}
-    ## Example with use OnNamespace strategy
-    # alertmanagerConfigMatcherStrategy:
-    #   type: OnNamespace
-
     ## Define Log Format
     # Use logfmt (default) or json logging
     logFormat: logfmt
@@ -1482,9 +708,6 @@
       org_role: Viewer
     auth.basic:
       enabled: false
-    dashboards:
-      # Modify this value to change the default dashboard shown on the main Grafana page
-      default_home_dashboard_path: /tmp/dashboards/rancher-default-home.json
     security:
       # Required to embed dashboards in Rancher Cluster Overview Dashboard on Cluster Explorer
       allow_embedding: true
@@ -1565,10 +788,7 @@
     dashboards:
       enabled: true
       label: grafana_dashboard
-      searchNamespace: cattle-dashboards
       labelValue: "1"
-      # Allow discovery in all namespaces for dashboards
-      searchNamespace: ALL
 
       ## Annotations for Grafana dashboard configmaps
       ##
@@ -1576,14 +796,11 @@
       multicluster:
         global:
           enabled: false
-        etcd:
-          enabled: false
       provider:
         allowUiUpdates: false
     datasources:
       enabled: true
       defaultDatasourceEnabled: true
-      isDefaultDatasource: true
 
       uid: prometheus
 
@@ -1591,9 +808,6 @@
       ##
       # url: http://prometheus-stack-prometheus:9090/
 
-      ## Prometheus request timeout in seconds
-      # timeout: 30
-
       # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
       # defaultDatasourceScrapeInterval: 15s
 
@@ -1601,14 +815,6 @@
       ##
       annotations: {}
 
-      ## Set method for HTTP to send query to datasource
-      httpMethod: POST
-
-      ## Create datasource for each Pod of Prometheus StatefulSet;
-      ## this uses headless service `prometheus-operated` which is
-      ## created by Prometheus Operator
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286
-      createPrometheusReplicasDatasources: false
       label: grafana_datasource
       labelValue: "1"
 
@@ -1726,690 +932,8 @@
     tlsConfig: {}
     scrapeTimeout: 30s
 
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-  resources:
-    limits:
-      memory: 200Mi
-      cpu: 200m
-    requests:
-      memory: 100Mi
-      cpu: 100m
-
-  testFramework:
-    enabled: false
-
-## Flag to disable all the kubernetes component scrapers
-##
-kubernetesServiceMonitors:
-  enabled: true
-
-## Component scraping the kube api server
-##
-kubeApiServer:
-  enabled: true
-  tlsConfig:
-    serverName: kubernetes
-    insecureSkipVerify: false
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    jobLabel: component
-    selector:
-      matchLabels:
-        component: apiserver
-        provider: kubernetes
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings:
-      # Drop excessively noisy apiserver buckets.
-      - action: drop
-        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
-        sourceLabels:
-          - __name__
-          - le
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels:
-    #     - __meta_kubernetes_namespace
-    #     - __meta_kubernetes_service_name
-    #     - __meta_kubernetes_endpoint_port_name
-    #   action: keep
-    #   regex: default;kubernetes;https
-    # - targetLabel: __address__
-    #   replacement: kubernetes.default.svc:443
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping the kubelet and kubelet-hosted cAdvisor
-##
-kubelet:
-  enabled: true
-  namespace: kube-system
-
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## Enable scraping the kubelet over https. For requirements to enable this see
-    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
-    ##
-    https: true
-
-    ## Enable scraping /metrics/cadvisor from kubelet's service
-    ##
-    cAdvisor: true
-
-    ## Enable scraping /metrics/probes from kubelet's service
-    ##
-    probes: true
-
-    ## Enable scraping /metrics/resource from kubelet's service
-    ## This is disabled by default because container metrics are already exposed by cAdvisor
-    ##
-    resource: false
-    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
-    resourcePath: "/metrics/resource/v1alpha1"
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    cAdvisorMetricRelabelings:
-      # Drop less useful container CPU metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
-      # Drop less useful container / always zero filesystem metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
-      # Drop less useful / always zero container memory metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_memory_(mapped_file|swap)'
-      # Drop less useful container process metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_(file_descriptors|tasks_state|threads_max)'
-      # Drop container spec metrics that overlap with kube-state-metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_spec.*'
-      # Drop cgroup metrics with no pod.
-      - sourceLabels: [id, pod]
-        action: drop
-        regex: '.+;'
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    probesMetricRelabelings: []
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    ## metrics_path is required to match upstream rules and charts
-    cAdvisorRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    probesRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    resourceRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    ## metrics_path is required to match upstream rules and charts
-    relabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping the kube controller manager
-##
-kubeControllerManager:
-  enabled: false
-
-  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## If using kubeControllerManager.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
-    ## of default port in Kubernetes 1.22.
-    ##
-    port: null
-    targetPort: null
-    # selector:
-    #   component: kube-controller-manager
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## Enable scraping kube-controller-manager over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
-    ##
-    https: null
-
-    # Skip TLS certificate validation when scraping
-    insecureSkipVerify: null
-
-    # Name of the server to use when validating TLS certificate
-    serverName: null
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping coreDns. Use either this or kubeDns
-##
-coreDns:
-  enabled: true
-  service:
-    port: 9153
-    targetPort: 9153
-    # selector:
-    #   k8s-app: kube-dns
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kubeDns. Use either this or coreDns
-##
-kubeDns:
-  enabled: false
-  service:
-    dnsmasq:
-      port: 10054
-      targetPort: 10054
-    skydns:
-      port: 10055
-      targetPort: 10055
-    # selector:
-    #   k8s-app: kube-dns
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    dnsmasqMetricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    dnsmasqRelabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping etcd
-##
-kubeEtcd:
-  enabled: false
-
-  ## If your etcd is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    port: 2381
-    targetPort: 2381
-    # selector:
-    #   component: etcd
-
-  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
-  ## specifying security configuration below. For example, with a secret named etcd-client-cert
-  ##
-  ## serviceMonitor:
-  ##   scheme: https
-  ##   insecureSkipVerify: false
-  ##   serverName: localhost
-  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
-  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
-  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
-  ##
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-    scheme: http
-    insecureSkipVerify: false
-    serverName: ""
-    caFile: ""
-    certFile: ""
-    keyFile: ""
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube scheduler
-##
-kubeScheduler:
-  enabled: false
-
-  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## If using kubeScheduler.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
-    ## of default port in Kubernetes 1.23.
-    ##
-    port: null
-    targetPort: null
-    # selector:
-    #   component: kube-scheduler
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-    ## Enable scraping kube-scheduler over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
-    ##
-    https: null
-
-    ## Skip TLS certificate validation when scraping
-    insecureSkipVerify: null
-
-    ## Name of the server to use when validating TLS certificate
-    serverName: null
-
     ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
+    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
     ##
     metricRelabelings: []
     # - action: keep
@@ -2427,673 +951,27 @@
     #   replacement: $1
     #   action: replace
 
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube proxy
-##
-kubeProxy:
-  enabled: false
-
-  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  service:
-    enabled: true
-    port: 10249
-    targetPort: 10249
-    # selector:
-    #   k8s-app: kube-proxy
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## Enable scraping kube-proxy over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
-    ##
-    https: false
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube state metrics
-##
-kubeStateMetrics:
-  enabled: true
-
-## Configuration for kube-state-metrics subchart
-##
-kube-state-metrics:
-  namespaceOverride: ""
-  rbac:
-    create: true
-  releaseLabel: true
-  prometheus:
-    monitor:
-      enabled: true
-
-      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-      ##
-      interval: ""
-
-      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-      ##
-      sampleLimit: 0
-
-      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-      ##
-      targetLimit: 0
-
-      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelLimit: 0
-
-      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelNameLengthLimit: 0
-
-      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelValueLengthLimit: 0
-
-      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
-      ##
-      scrapeTimeout: ""
-
-      ## proxyUrl: URL of a proxy that should be used for scraping.
-      ##
-      proxyUrl: ""
-
-      # Keep labels from scraped data, overriding server-side labels
-      ##
-      honorLabels: true
-
-      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      metricRelabelings: []
-      # - action: keep
-      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-      #   sourceLabels: [__name__]
-
-      ## RelabelConfigs to apply to samples before scraping
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      relabelings: []
-      # - sourceLabels: [__meta_kubernetes_pod_node_name]
-      #   separator: ;
-      #   regex: ^(.*)$
-      #   targetLabel: nodename
-      #   replacement: $1
-      #   action: replace
-
-  selfMonitor:
-    enabled: false
-
-## Deploy node exporter as a daemonset to all nodes
-##
-nodeExporter:
-  enabled: true
-
-## Configuration for prometheus-node-exporter subchart
-##
-prometheus-node-exporter:
-  namespaceOverride: ""
-  podLabels:
-    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
-    ##
-    jobLabel: node-exporter
-  releaseLabel: true
-  extraArgs:
-    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
-    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
-  service:
-    portName: http-metrics
-  prometheus:
-    monitor:
-      enabled: true
-
-      jobLabel: jobLabel
-
-      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-      ##
-      interval: ""
-
-      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-      ##
-      sampleLimit: 0
-
-      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-      ##
-      targetLimit: 0
-
-      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelLimit: 0
-
-      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelNameLengthLimit: 0
-
-      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelValueLengthLimit: 0
-
-      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
-      ##
-      scrapeTimeout: ""
-
-      ## proxyUrl: URL of a proxy that should be used for scraping.
-      ##
-      proxyUrl: ""
-
-      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      metricRelabelings: []
-      # - sourceLabels: [__name__]
-      #   separator: ;
-      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
-      #   replacement: $1
-      #   action: drop
-
-      ## RelabelConfigs to apply to samples before scraping
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      relabelings: []
-      # - sourceLabels: [__meta_kubernetes_pod_node_name]
-      #   separator: ;
-      #   regex: ^(.*)$
-      #   targetLabel: nodename
-      #   replacement: $1
-      #   action: replace
-
-## Manages Prometheus and Alertmanager components
-##
-prometheusOperator:
-  enabled: true
-
-  ## Prometheus-Operator v0.39.0 and later support TLS natively.
-  ##
-  tls:
-    enabled: true
-    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
-    tlsMinVersion: VersionTLS13
-    # Users who are deploying this chart in GKE private clusters will need to add firewall rules to expose this port for admissions webhooks
-    internalPort: 8443
-
-  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
-  ## rules from making their way into prometheus and potentially preventing the container from starting
-  admissionWebhooks:
-    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
-    ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
-    failurePolicy:
-    ## The default timeoutSeconds is 10 and the maximum value is 30.
-    timeoutSeconds: 10
-    enabled: true
-    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
-    ## If unspecified, system trust roots on the apiserver are used.
-    caBundle: ""
-    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
-    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
-    ## certs ahead of time if you wish.
-    ##
-    annotations: {}
-    #   argocd.argoproj.io/hook: PreSync
-    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
-    patch:
-      enabled: true
-      image:
-        repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
-        tag: v20221220-controller-v1.5.1-58-g787ea74b6
-        sha: ""
-        pullPolicy: IfNotPresent
-      resources: {}
-      ## Provide a priority class name to the webhook patching job
-      ##
-      priorityClassName: ""
-      annotations: {}
-      #   argocd.argoproj.io/hook: PreSync
-      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
-      podAnnotations: {}
-      nodeSelector: {}
-      affinity: {}
-      tolerations: []
-
-      ## SecurityContext holds pod-level security attributes and common container settings.
-      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false
-      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
-      ##
-      securityContext:
-        runAsGroup: 2000
-        runAsNonRoot: true
-        runAsUser: 2000
-
-    # Security context for create job container
-    createSecretJob:
-      securityContext: {}
-
-      # Security context for patch job container
-    patchWebhookJob:
-      securityContext: {}
-
-    # Use certmanager to generate webhook certs
-    certManager:
-      enabled: false
-      # self-signed root certificate
-      rootCert:
-        duration: ""  # default to be 5y
-      admissionCert:
-        duration: ""  # default to be 1y
-      # issuerRef:
-      #   name: "issuer"
-      #   kind: "ClusterIssuer"
-
-  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
-  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
-  ##
-  namespaces: {}
-    # releaseNamespace: true
-    # additional:
-    # - kube-system
-
-  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
-  ##
-  denyNamespaces: []
-
-  ## Filter namespaces to look for prometheus-operator custom resources
-  ##
-  alertmanagerInstanceNamespaces: []
-  alertmanagerConfigNamespaces: []
-  prometheusInstanceNamespaces: []
-  thanosRulerInstanceNamespaces: []
-
-  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
-  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
-  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
-  ##
-  # clusterDomain: "cluster.local"
-
-  networkPolicy:
-    ## Enable creation of NetworkPolicy resources.
-    ##
-    enabled: false
-
-    ## Flavor of the network policy to use.
-    #  Can be:
-    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
-    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
-    flavor: kubernetes
-
-    # cilium:
-    #   egress:
-
-  ## Service account for Alertmanager to use.
-  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
-  ##
-  serviceAccount:
-    create: true
-    name: ""
-
-  ## Configuration for Prometheus operator service
-  ##
-  service:
-    annotations: {}
-    labels: {}
-    clusterIP: ""
-
-  ## Port to expose on each node
-  ## Only used if service.type is 'NodePort'
-  ##
-    nodePort: 30080
-
-    nodePortTls: 30443
-
-  ## Additional ports to open for Prometheus service
-  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
-  ##
-    additionalPorts: []
-
-  ## Loadbalancer IP
-  ## Only use if service.type is "LoadBalancer"
-  ##
-    loadBalancerIP: ""
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-  ## Service type
-  ## NodePort, ClusterIP, LoadBalancer
-  ##
-    type: ClusterIP
-
-    ## List of IP addresses at which the Prometheus server service is available
-    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
-    ##
-    externalIPs: []
-
-  # ## Labels to add to the operator deployment
-  # ##
-  labels: {}
-
-  ## Annotations to add to the operator deployment
-  ##
-  annotations: {}
-
-  ## Labels to add to the operator pod
-  ##
-  podLabels: {}
-
-  ## Annotations to add to the operator pod
-  ##
-  podAnnotations: {}
-
-  ## Assign a PriorityClassName to pods if set
-  # priorityClassName: ""
-
-  ## Define Log Format
-  # Use logfmt (default) or json logging
-  # logFormat: logfmt
-
-  ## Decrease log verbosity to errors only
-  # logLevel: error
-
-  ## If true, the operator will create and maintain a service for scraping kubelets
-  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
-  ##
-  kubeletService:
-    enabled: true
-    namespace: kube-system
-    ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
-    name: ""
-
-  ## Create a servicemonitor for the operator
-  ##
-  serviceMonitor:
-    ## Labels for ServiceMonitor
-    additionalLabels: {}
-
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
-    scrapeTimeout: ""
-    selfMonitor: true
-
-    ## Metric relabel configs to apply to samples before ingestion.
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    #   relabel configs to apply to samples before ingestion.
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-  ## Resource limits & requests
-  ##
   resources:
     limits:
+      memory: 200Mi
       cpu: 200m
-      memory: 500Mi
     requests:
-      cpu: 100m
       memory: 100Mi
+      cpu: 100m
 
-  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
-  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
-  ##
-  hostNetwork: false
-
-  ## Define which Nodes the Pods are scheduled on.
-  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
-  ##
-  nodeSelector: {}
-
-  ## Tolerations for use with node taints
-  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
-  ##
-  tolerations: []
-  # - key: "key"
-  #   operator: "Equal"
-  #   value: "value"
-  #   effect: "NoSchedule"
-
-  ## Assign custom affinity rules to the prometheus operator
-  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
-  ##
-  affinity: {}
-    # nodeAffinity:
-    #   requiredDuringSchedulingIgnoredDuringExecution:
-    #     nodeSelectorTerms:
-    #     - matchExpressions:
-    #       - key: kubernetes.io/e2e-az-name
-    #         operator: In
-    #         values:
-    #         - e2e-az1
-    #         - e2e-az2
-  dnsConfig: {}
-    # nameservers:
-    #   - 1.2.3.4
-    # searches:
-    #   - ns1.svc.cluster-domain.example
-    #   - my.dns.search.suffix
-    # options:
-    #   - name: ndots
-    #     value: "2"
-  #   - name: edns0
-  securityContext:
-    fsGroup: 65534
-    runAsGroup: 65534
-    runAsNonRoot: true
-    runAsUser: 65534
-
-  ## Container-specific security context configuration
-  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
-  ##
-  containerSecurityContext:
-    allowPrivilegeEscalation: false
-    readOnlyRootFilesystem: true
-
-  # Enable vertical pod autoscaler support for prometheus-operator
-  verticalPodAutoscaler:
+  testFramework:
     enabled: false
-    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
-    controlledResources: []
-
-    # Define the max allowed resources for the pod
-    maxAllowed: {}
-    # cpu: 200m
-    # memory: 100Mi
-    # Define the min allowed resources for the pod
-    minAllowed: {}
-    # cpu: 200m
-    # memory: 100Mi
-
-    updatePolicy:
-      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
-      # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
-      updateMode: Auto
-
-  ## Prometheus-operator image
-  ##
-  image:
-    repository: rancher/mirrored-prometheus-operator-prometheus-operator
-    tag: v0.65.1
-    sha: ""
-    pullPolicy: IfNotPresent
-
-  ## Prometheus image to use for prometheuses managed by the operator
-  ##
-  # prometheusDefaultBaseImage: prometheus/prometheus
-
-  ## Prometheus image registry to use for prometheuses managed by the operator
-  ##
-  # prometheusDefaultBaseImageRegistry: quay.io
-
-  ## Alertmanager image to use for alertmanagers managed by the operator
-  ##
-  # alertmanagerDefaultBaseImage: prometheus/alertmanager
-
-  ## Alertmanager image registry to use for alertmanagers managed by the operator
-  ##
-  # alertmanagerDefaultBaseImageRegistry: quay.io
-
-  ## Prometheus-config-reloader
-  ##
-  prometheusConfigReloader:
-    image:
-      repository: rancher/mirrored-prometheus-operator-prometheus-config-reloader
-      tag: v0.65.1
-      sha: ""
-
-    # add prometheus config reloader liveness and readiness probe. Default: false
-    enableProbe: false
-
-    # resource config for prometheusConfigReloader
-    resources:
-      requests:
-        cpu: 200m
-        memory: 50Mi
-      limits:
-        cpu: 200m
-        memory: 50Mi
-
-  ## Thanos side-car image when configured
-  ##
-  thanosImage:
-    repository: rancher/mirrored-thanos-thanos
-    tag: v0.30.2
-    sha: ""
-
-  ## Set a Label Selector to filter watched prometheus and prometheusAgent
-  ##
-  prometheusInstanceSelector: ""
-
-  ## Set a Label Selector to filter watched alertmanager
-  ##
-  alertmanagerInstanceSelector: ""
-
-  ## Set a Label Selector to filter watched thanosRuler
-  thanosRulerInstanceSelector: ""
-
-  ## Set a Field Selector to filter watched secrets
-  ##
-  secretFieldSelector: "type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1"
 
 ## Deploy a Prometheus instance
 ##
 prometheus:
+
   enabled: true
 
   ## Annotations for Prometheus
   ##
   annotations: {}
 
-  ## Configure network policy for the prometheus
-  networkPolicy:
-    enabled: false
-
-    ## Flavor of the network policy to use.
-    #  Can be:
-    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
-    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
-    flavor: kubernetes
-
-    # cilium:
-    #   endpointSelector:
-    #   egress:
-    #   ingress:
-
-    # egress:
-    # - {}
-    # ingress:
-    # - {}
-    # podSelector:
-    #   matchLabels:
-    #     app: prometheus
-
   ## Service account for Prometheuses to use.
   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
   ##
@@ -3102,100 +980,6 @@
     name: ""
     annotations: {}
 
-  # Service for thanos service discovery on sidecar
-  # Enable this can make Thanos Query can use
-  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
-  # Thanos sidecar on prometheus nodes
-  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
-  thanosService:
-    enabled: false
-    annotations: {}
-    labels: {}
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-    ## Service type
-    ##
-    type: ClusterIP
-
-    ## gRPC port config
-    portName: grpc
-    port: 10901
-    targetPort: "grpc"
-
-    ## HTTP port config (for metrics)
-    httpPortName: http
-    httpPort: 10902
-    targetHttpPort: "http"
-
-    ## ClusterIP to assign
-    # Default is to make this a headless service ("None")
-    clusterIP: "None"
-
-    ## Port to expose on each node, if service type is NodePort
-    ##
-    nodePort: 30901
-    httpNodePort: 30902
-
-  # ServiceMonitor to scrape Sidecar metrics
-  # Needs thanosService to be enabled as well
-  thanosServiceMonitor:
-    enabled: false
-    interval: ""
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-
-    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
-    scheme: ""
-
-    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
-    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
-    tlsConfig: {}
-
-    bearerTokenFile:
-
-    ## Metric relabel configs to apply to samples before ingestion.
-    metricRelabelings: []
-
-    ## relabel configs to apply to samples before ingestion.
-    relabelings: []
-
-  # Service for external access to sidecar
-  # Enabling this creates a service to expose thanos-sidecar outside the cluster.
-  thanosServiceExternal:
-    enabled: false
-    annotations: {}
-    labels: {}
-    loadBalancerIP: ""
-    loadBalancerSourceRanges: []
-
-    ## gRPC port config
-    portName: grpc
-    port: 10901
-    targetPort: "grpc"
-
-    ## HTTP port config (for metrics)
-    httpPortName: http
-    httpPort: 10902
-    targetHttpPort: "http"
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-    ## Service type
-    ##
-    type: LoadBalancer
-
-    ## Port to expose on each node
-    ##
-    nodePort: 30901
-    httpNodePort: 30902
-
   ## Configuration for Prometheus service
   ##
   service:
@@ -3246,36 +1030,6 @@
 
     sessionAffinity: ""
 
-  ## Configuration for creating a separate Service for each statefulset Prometheus replica
-  ##
-  servicePerReplica:
-    enabled: false
-    annotations: {}
-
-    ## Port for Prometheus Service per replica to listen on
-    ##
-    port: 9090
-
-    ## To be used with a proxy extraContainer port
-    targetPort: 9090
-
-    ## Port to expose on each node
-    ## Only used if servicePerReplica.type is 'NodePort'
-    ##
-    nodePort: 30091
-
-    ## Loadbalancer source IP ranges
-    ## Only used if servicePerReplica.type is "LoadBalancer"
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-    ## Service type
-    ##
-    type: ClusterIP
-
   ## Configure pod disruption budgets for Prometheus
   ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
   ## This configuration is immutable once created and will require the PDB to be deleted to be changed
@@ -3286,46 +1040,6 @@
     minAvailable: 1
     maxUnavailable: ""
 
-  # Ingress exposes thanos sidecar outside the cluster
-  thanosIngress:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-    servicePort: 10901
-
-    ## Port to expose on each node
-    ## Only used if service.type is 'NodePort'
-    ##
-    nodePort: 30901
-
-    ## Hosts must be provided if Ingress is enabled.
-    ##
-    hosts: []
-      # - thanos-gateway.domain.com
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## TLS configuration for Thanos Ingress
-    ## Secret must be manually created in the namespace
-    ##
-    tls: []
-    # - secretName: thanos-gateway-tls
-    #   hosts:
-    #   - thanos-gateway.domain.com
-    #
-
   ## ExtraSecret can be used to store various data in an extra secret
   ## (use it for example to store hashed basic auth credentials)
   extraSecret:
@@ -3374,55 +1088,10 @@
       #   hosts:
       #     - prometheus.example.com
 
-  ## Configuration for creating an Ingress that will map to each Prometheus replica service
-  ## prometheus.servicePerReplica must be enabled
-  ##
-  ingressPerReplica:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-
-    ## Final form of the hostname for each per replica ingress is
-    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
-    ##
-    ## Prefix for the per replica ingress that will have `-$replicaNumber`
-    ## appended to the end
-    hostPrefix: ""
-    ## Domain that will be used for the per replica ingress
-    hostDomain: ""
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## Secret name containing the TLS certificate for Prometheus per replica ingress
-    ## Secret must be manually created in the namespace
-    tlsSecretName: ""
-
-    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
-    ##
-    tlsSecretPerReplica:
-      enabled: false
-      ## Final form of the secret for each per replica ingress is
-      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
-      ##
-      prefix: "prometheus"
-
   ## Configure additional options for default pod security policy for Prometheus
   ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
   podSecurityPolicy:
     allowedCapabilities: []
-    allowedHostPaths: []
     volumes: []
 
   serviceMonitor:
@@ -3431,35 +1100,11 @@
     interval: ""
     selfMonitor: true
 
-    ## Additional labels
-    ##
-    additionalLabels: {}
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
     ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
     scheme: ""
 
     ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
-    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
+    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
     tlsConfig: {}
 
     bearerTokenFile:
@@ -3493,15 +1138,11 @@
     ##
     apiserverConfig: {}
 
-    ## Allows setting additional arguments for the Prometheus container
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus
-    additionalArgs: []
-
     ## Interval between consecutive scrapes.
     ## Defaults to 30s.
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
     ##
-    scrapeInterval: ""
+    scrapeInterval: "30s"
 
     ## Number of seconds to wait for target to respond before erroring
     ##
@@ -3509,7 +1150,7 @@
 
     ## Interval between consecutive evaluations.
     ##
-    evaluationInterval: ""
+    evaluationInterval: "1m"
 
     ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
     ##
@@ -3521,10 +1162,6 @@
     ##
     enableAdminAPI: false
 
-    ## Sets version of Prometheus overriding the Prometheus version as derived
-    ## from the image tag. Useful in cases where the tag does not follow semver v2.
-    version: ""
-
     ## WebTLSConfig defines the TLS parameters for HTTPS
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig
     web: {}
@@ -3546,7 +1183,7 @@
     ##
     image:
       repository: rancher/mirrored-prometheus-prometheus
-      tag: v2.42.0
+      tag: v2.48.1
       sha: ""
 
     ## Tolerations for use with node taints
@@ -3592,14 +1229,6 @@
     ##
     enableRemoteWriteReceiver: false
 
-    ## Name of the external label used to denote replica name
-    ##
-    replicaExternalLabelName: ""
-
-    ## If true, the Operator won't add the external label used to denote replica name
-    ##
-    replicaExternalLabelNameClear: false
-
     ## Name of the external label used to denote Prometheus instance name
     ##
     prometheusExternalLabelName: ""
@@ -3634,13 +1263,6 @@
     ##
     query: {}
 
-    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
-    ruleNamespaceSelector: {}
-    ## Example which selects PrometheusRules in namespaces with label "prometheus" set to "somelabel"
-    # ruleNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
     ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the PrometheusRule resources created
@@ -3650,21 +1272,13 @@
     ## PrometheusRules to be selected for target discovery.
     ## If {}, select all PrometheusRules
     ##
-    ruleSelector: {}
-    ## Example which select all PrometheusRules resources
-    ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
-    # ruleSelector:
-    #   matchExpressions:
-    #     - key: prometheus
-    #       operator: In
-    #       values:
-    #         - example-rules
-    #         - example-rules-2
-    #
-    ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
-    # ruleSelector:
-    #   matchLabels:
-    #     role: example-rules
+    ruleSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -3675,19 +1289,13 @@
     ## ServiceMonitors to be selected for target discovery.
     ## If {}, select all ServiceMonitors
     ##
-    serviceMonitorSelector: {}
-    ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
-    # serviceMonitorSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## Namespaces to be selected for ServiceMonitor discovery.
-    ##
-    serviceMonitorNamespaceSelector: {}
-    ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
-    # serviceMonitorNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    serviceMonitorSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -3698,18 +1306,13 @@
     ## PodMonitors to be selected for target discovery.
     ## If {}, select all PodMonitors
     ##
-    podMonitorSelector: {}
-    ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
-    # podMonitorSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.
-    podMonitorNamespaceSelector: {}
-    ## Example which selects PodMonitor in namespaces with label "prometheus" set to "somelabel"
-    # podMonitorNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    podMonitorSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -3720,18 +1323,13 @@
     ## Probes to be selected for target discovery.
     ## If {}, select all Probes
     ##
-    probeSelector: {}
-    ## Example which selects Probes with label "prometheus" set to "somelabel"
-    # probeSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.
-    probeNamespaceSelector: {}
-    ## Example which selects Probe in namespaces with label "prometheus" set to "somelabel"
-    # probeNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    probeSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## How long to retain metrics
     ##
@@ -3739,12 +1337,7 @@
 
     ## Maximum size of metrics
     ##
-    retentionSize: ""
-
-    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration
-    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
-    tsdb:
-      outOfOrderTimeWindow: 0s
+    retentionSize: "50GiB"
 
     ## Enable compression of the write-ahead log using Snappy.
     ##
@@ -3814,13 +1407,6 @@
     #         - e2e-az1
     #         - e2e-az2
 
-    ## The remote_read spec configuration for Prometheus.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec
-    remoteRead: []
-    # - url: http://remote1/read
-    ## additionalRemoteRead is appended to remoteRead
-    additionalRemoteRead: []
-
     ## The remote_write spec configuration for Prometheus.
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
     remoteWrite: []
@@ -3841,6 +1427,8 @@
         memory: 750Mi
         cpu: 750m
 
+    storage:
+      enabled: false
     ## Prometheus StorageSpec for persistent data
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
     ##
